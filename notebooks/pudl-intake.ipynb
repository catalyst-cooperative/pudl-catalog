{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd Party Imports:\n",
    "import pandas as pd\n",
    "from intake import open_catalog\n",
    "from pudl.output.epacems import year_state_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter(\"%(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential benefits of Intake catalogs:\n",
    "**Expose metadata:** The Intake catalog doesn't contain any column-level metadata, but I think it could. This would allow a user to see what columns were available, what their types were, and read their descriptions before querying the large dataset.\n",
    "\n",
    "**Local data caching:** Local file caching is not available. We would expect this to make using many small files more efficient for repeated access, since they would each only need to be transmitted over the network once. However, `fsspec` based file caching hasn't yet been implemented in the `intake-parquet` library.\n",
    "\n",
    "**Data packaging:** The catalog can be packaged and versioned using conda to manage its dependencies on other software packages and ensure compatibility. With remote access or automatic local file caching, the user also doesn't need to think about where the data is being stored, or putting it in the \"right place\" -- Intake would manage that.\n",
    "\n",
    "**Uniform API:** All the data sources of a given type (parquet, SQL) would have the same interface, reducing the number of things a user needs to remember to access the data.\n",
    "\n",
    "**Decoupling data storage location:** As with DNS, we can change / update the location where the data is being stored without impacting the user directly, since the catalog acts as a decoupling reference.\n",
    "\n",
    "## Intake References\n",
    "* [Intake Documentations](https://intake.readthedocs.io/en/latest/start.html)\n",
    "* [Intake Examples](https://github.com/intake/intake-examples)\n",
    "* [CarbonPlan Data Catalogs](https://github.com/carbonplan/data)\n",
    "* [AnacondaCon Presentation Video](https://www.youtube.com/watch?v=oyZJrROQzUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Intake & Parquet Functionality & Performance\n",
    "\n",
    "This notebook demonstrates several different ways of organizing and accessing the same EPA CEMS data:\n",
    "* Local storage on disk vs. remote storage in Google Cloud Storage buckets\n",
    "* Directly accessing the data via `pandas.read_parquet()` vs. an Intake catalog.\n",
    "* Using one big Parquet file for all data vs. separate small files for each combination of state & year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_YEARS = [2019, 2020]\n",
    "TEST_STATES = [\"ID\", \"CO\", \"TX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_state_filter(years=(), states=()):\n",
    "    \"\"\"\n",
    "    Create filters to read given years and states from partitioned parquet dataset.\n",
    "\n",
    "    A subset of an Apache Parquet dataset can be read in more efficiently if files\n",
    "    which don't need to be queried are avoideed. Some datasets are partitioned based\n",
    "    on the values of columns to make this easier. The EPA CEMS dataset which we\n",
    "    publish is partitioned by state and report year.\n",
    "\n",
    "    However, the way the filters are specified can be unintuitive. They use DNF\n",
    "    (disjunctive normal form) See this blog post for more details:\n",
    "\n",
    "    https://blog.datasyndrome.com/python-and-parquet-performance-e71da65269ce\n",
    "\n",
    "    This function takes a set of years, and a set of states, and returns a list of lists\n",
    "    of tuples, appropriate for use with the read_parquet() methods of pandas and dask\n",
    "    dataframes. The filter will include all combinations of the specified years and\n",
    "    states. E.g. if years=(2018, 2019) and states=(\"CA\", \"CO\") then the filter would\n",
    "    result in getting 2018 and 2019 data for CO, as well as 2018 and 2019 data for CA.\n",
    "\n",
    "    Args:\n",
    "        years (iterable): 4-digit integers indicating the years of data you would like\n",
    "            to read. By default it includes all years.\n",
    "        states (iterable): 2-letter state abbreviations indicating what states you would\n",
    "            like to include. By default it includes all states.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists of tuples, suitable for use as a filter in the\n",
    "        read_parquet method of pandas and dask dataframes.\n",
    "\n",
    "    \"\"\"\n",
    "    year_filters = [(\"year\", \"=\", year) for year in years]\n",
    "    state_filters = [(\"state\", \"=\", state.upper()) for state in states]\n",
    "\n",
    "    if states and not years:\n",
    "        filters = [\n",
    "            [\n",
    "                tuple(x),\n",
    "            ]\n",
    "            for x in state_filters\n",
    "        ]\n",
    "    elif years and not states:\n",
    "        filters = [\n",
    "            [\n",
    "                tuple(x),\n",
    "            ]\n",
    "            for x in year_filters\n",
    "        ]\n",
    "    elif years and states:\n",
    "        filters = [list(x) for x in product(year_filters, state_filters)]\n",
    "    else:\n",
    "        filters = None\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestEpaCemsParquet(object):\n",
    "    table_name: str = \"hourly_emissions_epacems\"\n",
    "    gcs_base: str = \"gcs://catalyst.coop/intake/test\"\n",
    "    https_base: str = \"https://storage.googleapis.com/catalyst.coop/intake/test\"\n",
    "    local_base: str = os.getcwd() + \"/\"\n",
    "    pudl_catalog_yml: str = \"../src/catalog/pudl-catalog.yml\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.base_paths = {\n",
    "            \"gcs\": self.gcs_base,\n",
    "            \"https\": self.https_base,\n",
    "            \"local\": self.local_base,\n",
    "        }\n",
    "\n",
    "    def urlpath(self, protocol: str, partition=False):\n",
    "        if partition:\n",
    "            assert protocol != \"https\"\n",
    "\n",
    "        urlpath = self.base_paths[protocol] + \"/\" + self.table_name\n",
    "        if not partition:\n",
    "            urlpath += \".parquet\"\n",
    "        return urlpath\n",
    "    \n",
    "    def direct(self, protocol, partition, years, states):\n",
    "        filters = year_state_filter(years=years, states=states)\n",
    "        start_time = time.time()\n",
    "        urlpath =  self.urlpath(protocol=protocol, partition=partition)\n",
    "        logger.info(f\"read_parquet, {protocol=}, {partition=}, {years=}, {states=}:\")\n",
    "        df = pd.read_parquet(urlpath, filters=filters)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"    elapsed time: {elapsed_time:.2f}s\")\n",
    "        return df\n",
    "    \n",
    "    def intake(self, protocol, partition, years, states):\n",
    "        filters = year_state_filter(years=years, states=states)\n",
    "        os.environ[\"PUDL_INTAKE_PATH\"] = self.base_paths[protocol]\n",
    "        pudl_cat = open_catalog(self.pudl_catalog_yml)\n",
    "        if partition:\n",
    "            src = pudl_cat[\"hourly_emissions_epacems_partitioned\"](filters=filters)\n",
    "        else:\n",
    "            src = pudl_cat[\"hourly_emissions_epacems\"](filters=filters)\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"intake, {protocol=}, {partition=}, {years=}, {states=}:\")\n",
    "        df = src.to_dask().compute()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"    elapsed time: {elapsed_time:.2f}s\")\n",
    "        return df\n",
    "    \n",
    "    def test_direct(self, years, states, expected_df):\n",
    "        expected_df = self.direct(protocol=\"local\", partition=False, years=years, states=states)\n",
    "        direct_kwargs = [\n",
    "            dict(protocol=\"local\", partition=True),\n",
    "            dict(protocol=\"gcs\", partition=False),\n",
    "            dict(protocol=\"gcs\", partition=True),\n",
    "            # Tries to download the entire 4.7GB file and runs out of memory\n",
    "            # dict(protocol=\"https\", partition=False),\n",
    "            # https *must* refer to individual files. Can't list a directory or match patterns.\n",
    "            # dict(protocol=\"https\", partition=True),\n",
    "        ]       \n",
    "        for kwargs in direct_kwargs:\n",
    "            kwargs.update(dict(years=years, states=states))\n",
    "            test_df = self.direct(**kwargs)\n",
    "            pd.testing.assert_frame_equal(expected_df, test_df)\n",
    "\n",
    "    def test_intake(self, years, states):\n",
    "        expected_df = self.direct(protocol=\"local\", partition=False, years=years, states=states)\n",
    "        intake_kwargs = [\n",
    "            dict(protocol=\"local\", partition=False),\n",
    "            dict(protocol=\"local\", partition=True),\n",
    "            dict(protocol=\"gcs\", partition=False),\n",
    "            dict(protocol=\"gcs\", partition=True),\n",
    "            # Tries to download the entire 4.7GB file and runs out of memory\n",
    "            # dict(protocol=\"https\", partition=False),\n",
    "            # Results in a 403 Forbidden error on parquet file parent directory\n",
    "            #dict(protocol=\"https\", partition=True),\n",
    "        ]\n",
    "        for kwargs in intake_kwargs:\n",
    "            kwargs.update(dict(years=years, states=states))\n",
    "            test_df = self.intake(**kwargs)\n",
    "            logger.info(\"Verifying that dataframe matches expected output.\")\n",
    "            pd.testing.assert_frame_equal(expected_df, test_df)\n",
    "\n",
    "    def test_all(self, years, states):\n",
    "        self.test_direct(years, states)\n",
    "        self.test_intake(years, states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PUDL_INTAKE_CACHE\"] = str(Path.cwd() / \"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_parquet, protocol='local', partition=False, years=[2019, 2020], states=['ID', 'CO', 'TX']:\n",
      "    elapsed time: 2.43s\n",
      "intake, protocol='local', partition=False, years=[2019, 2020], states=['ID', 'CO', 'TX']:\n",
      "    elapsed time: 4.74s\n",
      "intake, protocol='local', partition=True, years=[2019, 2020], states=['ID', 'CO', 'TX']:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dude \u001b[38;5;241m=\u001b[39m TestEpaCemsParquet()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdude\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_intake\u001b[49m\u001b[43m(\u001b[49m\u001b[43myears\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEST_YEARS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEST_STATES\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mTestEpaCemsParquet.test_intake\u001b[0;34m(self, years, states)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kwargs \u001b[38;5;129;01min\u001b[39;00m intake_kwargs:\n\u001b[1;32m     79\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(years\u001b[38;5;241m=\u001b[39myears, states\u001b[38;5;241m=\u001b[39mstates))\n\u001b[0;32m---> 80\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     pd\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_frame_equal(expected_df, test_df)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mTestEpaCemsParquet.intake\u001b[0;34m(self, protocol, partition, years, states)\u001b[0m\n\u001b[1;32m     43\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     44\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintake, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotocol\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myears\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstates\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     47\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    elapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/pudl-dev/lib/python3.10/site-packages/dask/base.py:292\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dude = TestEpaCemsParquet()\n",
    "dude.test_intake(years=TEST_YEARS, states=TEST_STATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hourly_emissions_epacems', 'hourly_emissions_epacems_partitioned']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pudl_cat = open_catalog(dude.pudl_catalog_yml)\n",
    "list(pudl_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'creator': {'title': 'Catalyst Cooperative',\n",
       "  'email': 'pudl@catalyst.coop',\n",
       "  'path': 'https://catalyst.coop'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pudl_cat.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source level description\n",
    "* Basically from the YAML file, but with some extra things, `container`, `direct_access`, `user_parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'hourly_emissions_epacems',\n",
       " 'container': 'dataframe',\n",
       " 'plugin': ['parquet'],\n",
       " 'driver': ['parquet'],\n",
       " 'description': 'Hourly pollution emissions and plant operational data reported via Continuous Emissions Monitoring Systems (CEMS) as required by 40 CFR Part 75. Includes CO2, NOx, and SO2, as well as the heat content of fuel consumed and gross power output. Hourly values reported by US EIA ORISPL code and emissions unit (smokestack) ID.',\n",
       " 'direct_access': 'forbid',\n",
       " 'user_parameters': [],\n",
       " 'metadata': {'title': 'Continuous Emissions Monitoring System (CEMS) Hourly Data',\n",
       "  'type': 'application/parquet',\n",
       "  'provider': 'US Environmental Protection Agency Air Markets Program',\n",
       "  'path': 'https://ampd.epa.gov/ampd',\n",
       "  'license': {'name': 'CC-BY-4.0',\n",
       "   'title': 'Creative Commons Attribution 4.0',\n",
       "   'path': 'https://creativecommons.org/licenses/by/4.0'}},\n",
       " 'args': {'engine': 'pyarrow',\n",
       "  'urlpath': 'simplecache::{{ env(PUDL_INTAKE_PATH) }}/hourly_emissions_epacems.parquet',\n",
       "  'storage_options': {'simplecache': {'cache_storage': '{{ env(PUDL_INTAKE_CACHE) }}'}}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pudl_cat.hourly_emissions_epacems.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source internals\n",
    "* Categorical values showing up as integers.\n",
    "* String values showing up as objects.\n",
    "* No length in the shape, but 19 columns.\n",
    "* `npartitions` is apparently referring to file, not row-group based partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtype': {'plant_id_eia': 'int32',\n",
       "  'unitid': 'object',\n",
       "  'operating_datetime_utc': 'datetime64[ns, UTC]',\n",
       "  'year': 'int32',\n",
       "  'state': 'int64',\n",
       "  'facility_id': 'int32',\n",
       "  'unit_id_epa': 'object',\n",
       "  'operating_time_hours': 'float32',\n",
       "  'gross_load_mw': 'float32',\n",
       "  'heat_content_mmbtu': 'float32',\n",
       "  'steam_load_1000_lbs': 'float32',\n",
       "  'so2_mass_lbs': 'float32',\n",
       "  'so2_mass_measurement_code': 'int64',\n",
       "  'nox_rate_lbs_mmbtu': 'float32',\n",
       "  'nox_rate_measurement_code': 'int64',\n",
       "  'nox_mass_lbs': 'float32',\n",
       "  'nox_mass_measurement_code': 'int64',\n",
       "  'co2_mass_tons': 'float32',\n",
       "  'co2_mass_measurement_code': 'int64'},\n",
       " 'shape': (None, 19),\n",
       " 'npartitions': 1,\n",
       " 'metadata': {'title': 'Continuous Emissions Monitoring System (CEMS) Hourly Data',\n",
       "  'type': 'application/parquet',\n",
       "  'provider': 'US Environmental Protection Agency Air Markets Program',\n",
       "  'path': 'https://ampd.epa.gov/ampd',\n",
       "  'license': {'name': 'CC-BY-4.0',\n",
       "   'title': 'Creative Commons Attribution 4.0',\n",
       "   'path': 'https://creativecommons.org/licenses/by/4.0'},\n",
       "  'catalog_dir': '/home/zane/code/catalyst/pudl-data-catalog/notebooks/../src/catalog/'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pudl_cat.hourly_emissions_epacems.discover()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The other source internals\n",
    "* Here we have nullable ints, but they're 64-bit?\n",
    "* Categories show up as `category` not integers.\n",
    "* Strings show up as `string` not `object`\n",
    "* `npartitions` is referring to the separate files. How do we get information about how the partitions are structured in here?\n",
    "* Somehow in `shape` we've lost a couple of columns! There's no state or year. Probably this is because of how I converted from the old hive partitioned version of epacems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtype': {'plant_id_eia': 'int32',\n",
       "  'unitid': 'object',\n",
       "  'operating_datetime_utc': 'datetime64[ns, UTC]',\n",
       "  'year': 'int32',\n",
       "  'state': 'int64',\n",
       "  'facility_id': 'int32',\n",
       "  'unit_id_epa': 'object',\n",
       "  'operating_time_hours': 'float32',\n",
       "  'gross_load_mw': 'float32',\n",
       "  'heat_content_mmbtu': 'float32',\n",
       "  'steam_load_1000_lbs': 'float32',\n",
       "  'so2_mass_lbs': 'float32',\n",
       "  'so2_mass_measurement_code': 'int64',\n",
       "  'nox_rate_lbs_mmbtu': 'float32',\n",
       "  'nox_rate_measurement_code': 'int64',\n",
       "  'nox_mass_lbs': 'float32',\n",
       "  'nox_mass_measurement_code': 'int64',\n",
       "  'co2_mass_tons': 'float32',\n",
       "  'co2_mass_measurement_code': 'int64'},\n",
       " 'shape': (None, 19),\n",
       " 'npartitions': 1274,\n",
       " 'metadata': {'title': 'Continuous Emissions Monitoring System (CEMS) Hourly Data',\n",
       "  'type': 'application/parquet',\n",
       "  'provider': 'US Environmental Protection Agency Air Markets Program',\n",
       "  'path': 'https://ampd.epa.gov/ampd',\n",
       "  'license': {'name': 'CC-BY-4.0',\n",
       "   'title': 'Creative Commons Attribution 4.0',\n",
       "   'path': 'https://creativecommons.org/licenses/by/4.0'},\n",
       "  'catalog_dir': '/home/zane/code/catalyst/pudl-data-catalog/notebooks/../src/catalog/'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pudl_cat.hourly_emissions_epacems_partitioned.discover()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUDL Baseline\n",
    "Read the test data from your local EPA CEMS outputs directly.\n",
    "* On an SSD this should take less than 10 seconds.\n",
    "* The only `string` type columns should be `unitid` and `unit_id_epa`\n",
    "* The dataframe should take about 1.4 GB of memory and have ~8M rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_parquet, protocol='local', partition=False, years=[2019, 2020], states=['ID', 'CO', 'TX']:\n",
      "    elapsed time: 2.57s\n",
      "CPU times: user 2.66 s, sys: 1.34 s, total: 3.99 s\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pudl_epacems = dude.direct(protocol=\"local\", partition=False, years=TEST_YEARS, states=TEST_STATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8006424 entries, 0 to 8006423\n",
      "Data columns (total 19 columns):\n",
      " #   Column                     Non-Null Count    Dtype              \n",
      "---  ------                     --------------    -----              \n",
      " 0   plant_id_eia               8006424 non-null  int32              \n",
      " 1   unitid                     8006424 non-null  object             \n",
      " 2   operating_datetime_utc     8006424 non-null  datetime64[ns, UTC]\n",
      " 3   year                       8006424 non-null  int32              \n",
      " 4   state                      8006424 non-null  category           \n",
      " 5   facility_id                8006424 non-null  int32              \n",
      " 6   unit_id_epa                8006424 non-null  object             \n",
      " 7   operating_time_hours       8003928 non-null  float32            \n",
      " 8   gross_load_mw              8006424 non-null  float32            \n",
      " 9   heat_content_mmbtu         8006424 non-null  float32            \n",
      " 10  steam_load_1000_lbs        33252 non-null    float32            \n",
      " 11  so2_mass_lbs               3586052 non-null  float32            \n",
      " 12  so2_mass_measurement_code  3586052 non-null  category           \n",
      " 13  nox_rate_lbs_mmbtu         3716001 non-null  float32            \n",
      " 14  nox_rate_measurement_code  3716001 non-null  category           \n",
      " 15  nox_mass_lbs               3716549 non-null  float32            \n",
      " 16  nox_mass_measurement_code  3716549 non-null  category           \n",
      " 17  co2_mass_tons              3688397 non-null  float32            \n",
      " 18  co2_mass_measurement_code  3688397 non-null  category           \n",
      "dtypes: category(5), datetime64[ns, UTC](1), float32(8), int32(3), object(2)\n",
      "memory usage: 1.3 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pudl_epacems.info(show_counts=True, memory_usage=\"deep\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File Local\n",
    "For the single file local tests, download [this file](https://storage.googleapis.com/catalyst.coop/intake/test/hourly_emissions_epacems.parquet) into the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* This takes 3-4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    local_single_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* This takes 3-4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = str(INTAKE_PATH_LOCAL)\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_one_file(filters=TEST_FILTERS)\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File Remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* Using the authenticated `gs://` URL it takes **20 seconds**\n",
    "* Using the public `https://` URL this takes **10+ minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    remote_single_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* With `gs://` URL this takes **1 minute**\n",
    "* With `https://` URL it downloads a huge amount of data and then times out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = INTAKE_PATH_REMOTE\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_one_file(filters=TEST_FILTERS)\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi File Local\n",
    "\n",
    "For the multi-file local tests download [this tarball](https://storage.googleapis.com/catalyst.coop/intake/test/hourly_emissions_epacems.tar) and extract it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* This takes 5 seconds, and results in an excessively large 3GB dataframe because I generated these parquet files before fixing the string-to-categorical type issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    local_multi_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* This takes about 15 seconds, and results in the 3GB dataframe as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = str(INTAKE_PATH_LOCAL)\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_multi_file(\n",
    "    filters=TEST_FILTERS,\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi File Remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct access with `read_parquet()`\n",
    "* With the `gs://` URL this takes **1 minute** and downloads minimal data.\n",
    "* With the `https://` URL this results in a 403 Forbidden error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\n",
    "    remote_multi_file,\n",
    "    engine=\"pyarrow\",\n",
    "    filters=TEST_FILTERS,\n",
    "    use_nullable_dtypes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Intake Catalog\n",
    "* With the `gs://` URL this takes **3 minutes** and downloads a little bit of data across the whole time.\n",
    "* With the `https://` URL this results in a 403 Forbidden error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ[\"INTAKE_PATH\"] = INTAKE_PATH_REMOTE\n",
    "pudl_cat = open_catalog(pudl_catalog_path)\n",
    "source = pudl_cat.epacems_multi_file(\n",
    "    filters=TEST_FILTERS,\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "dd = source.to_dask()\n",
    "df = dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info(show_counts=True, memory_usage=\"deep\"))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* Unsurprisingly, local access is blazing fast regardless of whether it's a single file or many, and while the Intake catalog access takes around 3x as long, it seems fast enough to be plenty usable.\n",
    "* Remote performance using a single file, the `gs://` protocol, and `read_parquet()` was shockingly fast. It took less than 10x as long as direct local access.\n",
    "* Remote performance over `https://` was painfully slow, to the point of being unusable in all uses of Intake. It also seemed to be transmitting far, far more data than in the `gs://` case.\n",
    "* Basically none of the `https://` cases were usable. The only one that completed took 10 minutes.\n",
    "* The only remote Intake catalog case that worked was the single-file `gs://`, which (as with the local catalogs) took about 3x as long as the `read_parquet()` case.\n",
    "* Over `https://` it seems like we can't use directories or wildcards -- we have to enumerate each filename specifically.\n",
    "* Some of the issues here have to be network speed, but I have 50-100Mbit download speeds, and the amount of data being transmitted varied widely between the different cases.\n",
    "* Still some data type issues happening in all of the Intake cases. Strings get turned into objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "* Can non-authenticated users access publicly readable data using `gs://` URLs?\n",
    "* How do we add column-level metadata to the catalog appropriately? Can we get the embedded descriptions to show up?\n",
    "* How do we add information about what's in the different partitions (i.e. split by year and state, allowable values)\n",
    "* Why are we getting jumbled nullable/non-nullable, ints/categories, strings/objects in the types?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
